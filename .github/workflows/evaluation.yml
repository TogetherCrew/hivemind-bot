name: RAG Evaluation

on:
  workflow_dispatch: {}

jobs:
  evaluate:
    runs-on: ubuntu-latest

    services:
      qdrant:
        image: qdrant/qdrant:latest
        ports:
          - 6333:6333

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          python -m spacy download en_core_web_sm

      - name: Wait for Qdrant to be ready
        run: |
          timeout 120 bash -c 'until curl -sf http://localhost:6333/collections >/dev/null; do echo "Waiting for Qdrant..."; sleep 3; done'

      - name: Run evaluation
        env:
          QDRANT_HOST: localhost
          QDRANT_PORT: '6333'
          QDRANT_API_KEY: ''
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          python evaluation/evaluation.py \
            --community-id 1234 \
            --platform-id 4321

      - name: Compute averages and summarize
        id: summarize
        run: |
          python - << 'PY'
          import os
          import pandas as pd
          import json
          from pathlib import Path

          results_csv = Path('results.csv')
          cost_json = Path('results_cost.json')

          assert results_csv.exists(), 'results.csv not found'
          df = pd.read_csv(results_csv)

          metrics = ['faithfulness','answer_relevancy','context_precision','context_recall']
          avgs = {m: float(df[m].mean()) for m in metrics if m in df.columns}

          cost_payload = {}
          if cost_json.exists():
            cost_payload = json.loads(cost_json.read_text())

          print('::group::Evaluation Averages')
          for k,v in avgs.items():
            print(f"{k}: {v:.4f}")
          print('::endgroup::')

          # Write GitHub Job Summary
          summary_lines = [
            '# Evaluation Summary',
            '',
            '| Metric | Average |',
            '|---|---:|',
          ]
          for k,v in avgs.items():
            summary_lines.append(f"| {k} | {v:.4f} |")

          if cost_payload:
            summary_lines += [
              '',
              '## Cost',
              f"- model: {cost_payload.get('model')}",
              f"- input_rate: {cost_payload.get('input_rate')}",
              f"- output_rate: {cost_payload.get('output_rate')}",
              f"- total_tokens: {cost_payload.get('total_tokens')}",
              f"- total_cost: {cost_payload.get('total_cost')}",
            ]

          Path(os.environ['GITHUB_STEP_SUMMARY']).write_text('\n'.join(summary_lines))
          PY

      - name: Upload results.csv
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: results-csv
          path: results.csv

      - name: Upload results_cost.json
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: results-cost
          path: results_cost.json


